{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb801c9-e33a-4a41-bdb8-9cacb382535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a8a43d-530e-4031-b42f-5b6bd09af34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfffcbf-d6e3-4e63-85dc-02fb916cee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sset up enviornment\n",
    "\n",
    "load_dotenv()\n",
    "openai=OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048e5e7c-dd7a-469e-9ed5-0c6f75fb0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d989ab-d1e2-4b93-9893-87c40ccde3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "user_prompt=\"Please give a detailed explanation to the following question: \" + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a02948-86cb-4adc-9d88-977e7ed99c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages\n",
    "\n",
    "messages=[\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6819c2cd-80e8-4cba-8472-b5a5729d2530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the given code snippet step by step to understand its functionality and purpose:\n",
       "\n",
       "### Code Explanation\n",
       "\n",
       "python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "1. **Context of `yield from`:**\n",
       "   - The keyword `yield` is used within a generator function in Python to yield values one at a time, allowing the function to produce a series of values lazily, meaning that values are generated only when requested.\n",
       "   - `yield from` is a special syntax in Python that delegates part of the generator’s operations to another iterable (in this case, the set comprehension). It is equivalent to yielding each item from the iterable one by one. This is useful for simplifying the code and avoiding nested loops when dealing with iterables.\n",
       "\n",
       "2. **Set Comprehension:**\n",
       "   - The part `{book.get(\"author\") for book in books if book.get(\"author\")}` is a *set comprehension*. Set comprehensions are syntactic constructs that allow for the creation of a set based on an iterable.\n",
       "   - Here’s what’s happening inside the set comprehension:\n",
       "     - `for book in books`: This iterates over each `book` in an iterable named `books`.\n",
       "     - `book.get(\"author\")`: This attempts to retrieve the value associated with the key `\"author\"` from each `book`. The `get` method of dictionaries returns `None` if the key does not exist, instead of raising a `KeyError`.\n",
       "     - `if book.get(\"author\")`: This condition filters the results. It only includes the `author` if it is truthy (i.e., it exists and is not `None`, `''` (empty string), or other values considered false in a boolean context). Therefore, it effectively excludes any books that do not have an author specified or where the author is `None`.\n",
       "\n",
       "3. **Resulting Set:**\n",
       "   - The output of the comprehension is a set of unique authors extracted from the `books` list where the author exists. If multiple books share the same author, that author's name will only appear once in the resulting set due to the nature of sets being collections of unique elements.\n",
       "\n",
       "### Overall Functionality\n",
       "\n",
       "Putting all of this together, the entire line of code does the following:\n",
       "\n",
       "- It iterates over a collection of `books`.\n",
       "- It attempts to access the `\"author\"` field of each book and only includes those authors that are valid (i.e., not `None` or empty).\n",
       "- It constructs a set of unique authors from this collection.\n",
       "- The `yield from` statement yields each author from the resulting set one at a time when the generator function is called.\n",
       "\n",
       "### Why Use This Code?\n",
       "\n",
       "- **Efficiency**: Using a set comprehension allows for the accumulation of unique author names efficiently and concisely.\n",
       "- **Memory Management**: By using generators and `yield from`, the code avoids creating a potentially large intermediate list, which is useful if `books` is large, as it produces authors on-the-fly.\n",
       "- **Readable and Pythonic**: The use of comprehensions and `yield from` is a common Python idiom that makes the code both concise and readable, adhering to Python's design philosophy of readability and simplicity.\n",
       "\n",
       "### Example Use Case\n",
       "\n",
       "If you had the following list of books:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book 3\"},  # No author\n",
       "    {\"title\": \"Book 4\", \"author\": \"Author A\"},  # Duplicate author\n",
       "    {\"title\": \"Book 5\", \"author\": None},  # No author\n",
       "]\n",
       "\n",
       "\n",
       "Using the provided line of code would yield:\n",
       "\n",
       "- `\"Author A\"`\n",
       "- `\"Author B\"`\n",
       "\n",
       "The duplicates are removed, and only valid authors are returned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "stream=openai.chat.completions.create(model=MODEL_GPT, messages=messages,stream=True)\n",
    "\n",
    "response=\"\"\n",
    "display_handle=display(Markdown(\"\"),display_id=True)\n",
    "for chunk in stream:\n",
    "    response +=chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(response),display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c15975-ba7d-4964-b94a-5ce105ccc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "reply = response['message']['content']\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0a013-c1f2-4f01-8b10-9f68325356e9",
   "metadata": {},
   "source": [
    "# Modify\n",
    "Update such that the question is taken as input and sent to the model for response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f01b258-a293-4afc-a99c-d3cfb624b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_responses(question):\n",
    "    \"\"\"\n",
    "    Takes a question as input, queries GPT-4o-mini and Llama 3.2 models, \n",
    "    and displays their responses.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to be processed by the models.\n",
    "    \"\"\"\n",
    "    # system_prompt is already declared above lets generate a new user prompt so that the input question can be sent\n",
    "    user_input_prompt = f\"Please give a detailed explanation to the following question: {question}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input_prompt}\n",
    "    ]\n",
    "     # GPT-4o-mini Response with Streaming\n",
    "    print(\"Fetching response from GPT-4o-mini...\")\n",
    "    stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, stream=True)\n",
    "\n",
    "    response_gpt = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response_gpt += chunk.choices[0].delta.content or ''\n",
    "        response_gpt = response_gpt.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response_gpt), display_id=display_handle.display_id)\n",
    "\n",
    "    # Llama 3.2 Response\n",
    "    print(\"Fetching response from Llama 3.2...\")\n",
    "    response_llama = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "    reply_llama = response_llama['message']['content']\n",
    "    display(Markdown(reply_llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd35ac5e-a934-4c20-9be9-657afef66c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question:  When did alexander the great die?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching response from GPT-4o-mini...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Alexander the Great died on June 10 or 11, 323 BCE, in the palace of Nebuchadnezzar II in Babylon (present-day Iraq). His death followed a period of intense military campaigning and expansion of his empire, which stretched from Greece to parts of Asia, including Egypt and India.\n",
       "\n",
       "Several historical accounts recount the circumstances surrounding his death. At the time of his demise, Alexander was just 32 years old. Various theories have been proposed regarding the cause of his death, including:\n",
       "\n",
       "1. **Natural Causes**: Some historians suggest that Alexander may have succumbed to a fever caused by malaria, typhoid, or another infectious disease. This conclusion is supported by the symptoms reported by ancient sources.\n",
       "\n",
       "2. **Poisoning**: Another theory posits that Alexander was poisoned, possibly by political rivals, though this is considered less likely by most modern historians because the symptoms of poisoning would likely have manifested much more quickly than the slow decline observed over several days.\n",
       "\n",
       "3. **Autoimmune Disorders**: Some modern medical experts speculate that Alexander may have had an autoimmune disorder, such as Guillain-Barré syndrome, which could explain his sudden illness and prolonged suffering before death.\n",
       "\n",
       "4. **Psychological Factors**: Stress and the pressure of his conquests might have also contributed to his health decline, as leaders often face significant mental strain, especially during times of war and political intrigue.\n",
       "\n",
       "Following his death, Alexander's empire began to fragment and was eventually divided among his generals during the Wars of the Diadochi. His legacy remains influential in history, philosophy, and military studies, symbolizing the archetype of a successful and ambitious leader."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching response from Llama 3.2...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "It seems there's a bit of a mismatch between the topic you're asking for (which is related to history) and my role as a technical tutor. However, I'll do my best to provide an answer while pointing out that this type of question might not be directly related to Python programming or software engineering.\n",
       "\n",
       "To provide a helpful response:\n",
       "\n",
       "Alexander the Great was a king who ruled over a vast portion of the known world at that time. He was born in 356 BCE and died in 323 BCE, after a long and illustrious career as a military commander and ruler. The exact cause of his death is not entirely certain, but it's believed to have been due to a fever or poisoning, possibly brought on by the many campaigns he had undertaken.\n",
       "\n",
       "If I were to rewrite this answer using a more technical tone (which might be a bit of an stretch!), I could say something like:\n",
       "\n",
       "\"Alexander the Great's mortality can be analyzed through the lens of historical records and epidemiological modeling. The dates of his birth (356 BCE) and death (323 BCE) provide a fixed reference point for calculating the time elapsed between these two events. Using Bayesian inference, we can attempt to estimate the probability of his cause of death being due to a fever or poisoning. However, it's essential to note that the accuracy of such an analysis relies heavily on the availability and quality of historical records.\"\n",
       "\n",
       "Please keep in mind that this answer is more of a creative exercise than a serious technical response. If you'd like me to explain something related to Python programming or software engineering, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Prompt user for their question\n",
    "my_question = input(\"Please enter your question: \")\n",
    "# Fetch and display responses from models\n",
    "get_model_responses(my_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e279c-29c0-4be0-aaec-a82269f77602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
